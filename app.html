<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Image Classifier test</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 20px; }
    .wrap { max-width: 720px; margin: 0 auto; }
    header { display:flex; align-items:center; gap:12px; margin-bottom:12px; }
    header a { text-decoration:none; }
    .row { display:flex; gap:12px; flex-wrap:wrap; align-items:center; margin:12px 0; }
    .status { margin-top: 8px; font-size: 14px; color: #555; }
    video, canvas, img { max-width:100%; border-radius:12px; box-shadow:0 2px 8px rgba(0,0,0,.12); }
    button, input[type=file] { padding:8px 12px; border-radius:10px; border:1px solid #ddd; background:#fff; cursor:pointer; }
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <a href="index.html">← Home</a>
      <h1 style="margin:0;">Image Classifier</h1>
    </header>

    <p>Select a photo or use your camera. The model predicts <strong>Fruit</strong> vs <strong>Vegetable</strong>.</p>

    <div class="row">
      <input id="file" type="file" accept="image/*">
      <button id="cameraBtn" disabled>Use Camera</button>
      <button id="stopBtn" disabled>Stop Camera</button>
    </div>

    <video id="video" playsinline autoplay muted style="display:none;"></video>
    <!-- Visible preview canvas (any size is fine) -->
    <canvas id="canvas" width="320" height="320" aria-label="preview"></canvas>
    <!-- Hidden 96x96 canvas that feeds the model -->
    <canvas id="prep" width="96" height="96" style="display:none"></canvas>

    <div class="status" id="label">Label: (loading model…)</div>
    <div class="status" id="prob">Confidence: —</div>

    <noscript><p><strong>JavaScript is required.</strong></p></noscript>
  </div>

  <!-- Load libraries FIRST (exact versions for compatibility) -->
  <script src="https://unpkg.com/@tensorflow/tfjs@1.7.4/dist/tf.min.js"></script>
  <script src="https://unpkg.com/@teachablemachine/image@0.8.4/dist/teachablemachine-image.min.js"></script>

  <!-- App script LAST (depends on the libraries above) -->
  <script>
    (function () {
      var MODEL_URL = './tm-model/';    // folder containing model.json, metadata.json, weights.bin

      var canvas = document.getElementById('canvas');
      var ctx = canvas.getContext('2d', { willReadFrequently: true });

      // Offscreen (hidden) 96x96 canvas that matches model input size
      var prep = document.getElementById('prep');
      var pctx = prep.getContext('2d', { willReadFrequently: true });

      var fileInput = document.getElementById('file');
      var cameraBtn = document.getElementById('cameraBtn');
      var stopBtn = document.getElementById('stopBtn');
      var video = document.getElementById('video');
      var labelEl = document.getElementById('label');
      var probEl = document.getElementById('prob');

      var tmModel = null;       // TeachableMachine wrapper
      var coreModel = null;     // underlying tf.LayersModel
      var classLabels = [];
      var webcamStream = null;
      var ready = false;

      function setStatus(labelText, probText) {
        labelEl.textContent = 'Label: ' + labelText;
        probEl.textContent = 'Confidence: ' + (probText || '—');
      }

      // Draw to visible preview AND to the 96x96 prep canvas
      function drawToCanvas(srcEl) {
        // visible preview (for the user)
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.drawImage(srcEl, 0, 0, canvas.width, canvas.height);

        // model input (exactly 96x96)
        pctx.clearRect(0, 0, prep.width, prep.height);
        pctx.drawImage(srcEl, 0, 0, prep.width, prep.height);
      }

      function enableInputs() {
        fileInput.disabled = false;
        cameraBtn.disabled = false;
      }

      function disableInputs() {
        fileInput.disabled = true;
        cameraBtn.disabled = true;
        stopBtn.disabled = true;
      }

      async function loadModel() {
        try {
          tmModel = await tmImage.load(MODEL_URL + 'model.json', MODEL_URL + 'metadata.json');
          coreModel = tmModel.model; // raw tf.LayersModel
          if (tmModel.getClassLabels) {
            classLabels = await tmModel.getClassLabels();
          }
          ready = true;
          setStatus('(model ready)', '—');
          enableInputs();
          console.log('Model loaded. Labels:', classLabels);
        } catch (err) {
          console.error('Model load failed:', err);
          setStatus('(model load failed)', (err && err.message) ? err.message : String(err));
          disableInputs();
        }
      }

      // Convert 96x96 RGB from 'prep' canvas -> grayscale tensor [1,96,96,1], then predict
      async function predictFromCanvas() {
        if (!ready || !coreModel) return;
        try {
          var input = tf.tidy(function () {
            // [96,96,3] 0..255
            var t = tf.browser.fromPixels(prep);
            var split = tf.split(t, 3, -1);        // [R,G,B]
            var r = split[0], g = split[1], b = split[2];
            // luminance Y = 0.299 R + 0.587 G + 0.114 B  -> [96,96,1]
            var y = r.mul(0.299).add(g.mul(0.587)).add(b.mul(0.114));
            return y.div(255).expandDims(0);       // [1,96,96,1]
          });

          var out = coreModel.predict(input);      // tf.Tensor
          var probs = await out.data();            // Float32Array
          input.dispose();
          out.dispose();

          if (!probs || probs.length === 0) {
            setStatus('(no predictions)', '—');
            return;
          }

          var topI = 0, topV = -1;
          for (var i = 0; i < probs.length; i++) {
            if (probs[i] > topV) { topV = probs[i]; topI = i; }
          }
          var lbl = classLabels[topI] || ('Class ' + topI);
          setStatus(lbl, (topV * 100).toFixed(1) + '%');
        } catch (err) {
          console.error('Predict error:', err);
          setStatus('(predict error)', (err && err.message) ? err.message : String(err));
        }
      }

      // File upload
      fileInput.addEventListener('change', function (e) {
        var file = e.target.files && e.target.files[0];
        if (!file) return;
        if (!ready) { setStatus('(model not ready)','—'); return; }
        setStatus('(processing…)','—');

        var img = new Image();
        img.onload = function () {
          drawToCanvas(img);
          predictFromCanvas();
          try { URL.revokeObjectURL(img.src); } catch (e) {}
        };
        img.onerror = function (err) {
          console.error('Image load error:', err);
          setStatus('(image load error)','—');
        };
        img.src = URL.createObjectURL(file);
      });

      // Camera on
      cameraBtn.addEventListener('click', async function () {
        if (!ready) return;
        try {
          webcamStream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' }, audio: false });
          video.srcObject = webcamStream;
          video.style.display = 'block';
          await video.play();

          function tick() {
            if (!webcamStream) return;
            drawToCanvas(video);
            predictFromCanvas();
            requestAnimationFrame(tick);
          }
          tick();
          stopBtn.disabled = false;
        } catch (err) {
          console.error('Camera error:', err);
          alert('Camera error: ' + err.message);
        }
      });

      // Camera off
      stopBtn.addEventListener('click', function () {
        if (webcamStream) {
          webcamStream.getTracks().forEach(function (t) { t.stop(); });
          webcamStream = null;
          video.style.display = 'none';
          stopBtn.disabled = true;
        }
      });

      // Start
      disableInputs(); // until model loads
      loadModel();
    })();
  </script>
</body>
</html>
